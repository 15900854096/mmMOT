# Robust Multi-Modality Multi-Object Tracking

This is the project page for our ICCV2019 paper: **Robust Multi-Modality Multi-Object Tracking**.

**Authors**: [Wenwei Zhang](http://zhangwenwei.cn), [Hui Zhou](https://scholar.google.com/citations?user=i35tdbMAAAAJ&hl=zh-CN), [Shuyang Sun](https://kevin-ssy.github.io/), [Zhe Wang](https://wang-zhe.me/), [Jianping Shi](http://shijianping.me/), [Chen Change Loy](http://personal.ie.cuhk.edu.hk/~ccloy/)

[[ArXiv]](https://arxiv.org/abs/1909.03850)&nbsp;  [[Project Page]](#)&nbsp;  [[Poster]](http://zhangwenwei.cn/files/mmMOT_poster_final.pdf)

## Introduction

In this work, we design a generic sensor-agnostic multi-modality MOT framework (mmMOT), where each modality (i.e., sensors) is capable of performing its role independently to preserve reliability, and further improving its accuracy through a novel multi-modality fusion module. Our mmMOT can be trained in an end-to-end manner, enables joint optimization for the base feature extractor of each modality and an adjacency estimator for cross modality. Our mmMOT also makes the first attempt to encode deep representation of point cloud in data association process in MOT. 

For more details, please refer our [paper](https://arxiv.org/abs/1909.03850).

## Codebase

We are still preparing the codebase for release. We will release the code and models as soon as possible.


## Citation

If you use this codebase or model in your research, please cite:
```
@InProceedings{mmMOT_2019_ICCV,
    author = {Zhang, Wenwei and Zhou, Hui and Sun, Shuyang, and Wang, Zhe and Shi, Jianping and Loy, Chen Change},
    title = {Robust Multi-Modality Multi-Object Tracking},
    booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
    month = {October},
    year = {2019}
}
```

